# AWS Data Engineering Fundamentals with S3, DynamoDB & Kinesis

This project offers a hands-on introduction to data engineering on AWS. The goal is to build a data pipeline leveraging various services to ingest, store, and manage data streams.

**Project Overview**

This project delves into the fundamentals of data engineering and its importance in today's data-driven world. I'm exploring: 

* Data types and data lakes
* Data sources and management with DynamoDB, Kinesis, and S3
* Building a data engineering pipeline for data ingestion and analysis

The tools used are :

* **Amazon S3:** Securely storing data, triggering notifications for actions, and managing data lifecycle with storage classes and lifecycle rules.
* **DynamoDB:** Understanding key features, partitioning tables for performance, and performing CRUD operations via the CLI.
* **Kinesis Data Streams:** Handling real-time data ingestion, putting data into streams, and processing data using consumers.
* **Firehose Delivery Streams:** Setting up a delivery stream for further data processing.

**Learning Objectives**

* Grasp data types, data lakes, and the data engineering pipeline.
* Master key features of DynamoDB, including partitioning for optimization.
* Create and manage DynamoDB tables using the Command Line Interface (CLI).
* Understand and utilize API actions for accessing and processing stream records.
* Configure ingestion patterns for specific use cases.
* Select the appropriate AWS service for data intake into the pipeline.
* Manage real-time data ingestion with Kinesis Data Streams.
* Work with Kinesis producers and consumers to put and process data.
* Configure a Firehose delivery stream for data transfer.
* Explore various data storage technologies based on usage and retrieval needs.
* Learn about S3 storage classes and configure lifecycle rules for data objects.
